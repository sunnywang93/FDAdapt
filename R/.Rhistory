}, simplify = "array")
regularise <- sapply(seq(nfunctions), function(j) {
sapply(seq_along(grid_bandwidth), function(h) {
regularise_st[,,h] * regularise_ts[,,j]
}, simplify = "array")
}, simplify = "array") |>
apply(MARGIN = c(2, 3, 4), function(s) pracma::trapz(grid_smooth, s)) |>
apply(MARGIN = c(2, 3), function(t) pracma::trapz(grid_smooth, t))
regularise_constant <- sapply(seq(nfunctions), function(j) {
1 / sum((evalues_norm[j] - evalues_norm[-j])**2)
})
regularise_term <- sweep(regularise, MARGIN = 2,
STATS = regularise_constant, FUN = "*")
risk <- bias_term + variance_term + regularise_term
min_h_index <- apply(risk, MARGIN = 2, which.min)
h_star <- grid_bandwidth[min_h_index]
h_Star
h_star
M_avg <- purrr::map_dbl(curves, ~length(.x$t)) |> mean()
M_avg
N_effective <- mean(cov_gkp$WN[,, min_h_index]) * M_avg
N_effective
h_constant <- log(N_effective)**(abs(log(h_star) / log(N_effective)))
h_constant
h_star * h_constant
load_all()
test = efunctions_adaptive(pfbm_mu, grid_bandwidth, grid_smooth, k0, 10, params)
plot(grid_smooth, test$eigenfunctions[, 1])
plot(grid_smooth, test$eigenfunctions[, 1], type = "l")
lines(efunctions_true[, 1])
plot(grid_smooth, efunctions_true[, 1])
plot(grid_smooth, efunctions_true[, 1], type = "l")
lines(grid_smooth, test$eigenfunction[, 1], col = "red")
gc()
source("/Users/swang/Dropbox/FPCA/simuls/nice_dgp/dgp.R")
#source("/Users/swang/Dropbox/Mac/Downloads/juin15/codes_val/dgp_old.R")
# source("/Users/swang/Dropbox/funeigen/R/H_and_L_functions.R")
# source("/Users/swang/Dropbox/funeigen/R/mean_optimised.R")
# source("/Users/swang/Dropbox/funeigen/R/cov_optimised.R")
# source("/Users/swang/Dropbox/funeigen/R/utils.R")
# source("/Users/swang/Dropbox/funeigen/R/eigenvalues.R")
library(foreach)
library(fs)
library(doParallel)
library(functional)
library(purrr)
#parameters
N <- 200
M <- 25
#H <- c(0.5, 0.8)
sigma <- 0.5
L <- 1
mu0 <- 1
grid_size_true <- 101
grid_true <- seq(0, 1, length.out = grid_size_true)
grid_mu <- seq(0, 1, length.out = 1001)
grid_smooth <- seq(0, 1, length.out = 101)
grid_bandwidth <- lseq(0.001, 0.1, length.out = 51)
grid_param = seq(0.1, 0.9, length.out = 20)
k0 <- 1
n_simu <- 45
# k_length <- 100
# alpha <- 2
#change_point <- 0.5
#slope <- 50
tau <- 2.5 #variance of random starting points
# scale <- 1
# shift <- 0
nvalues <- 10
points_dist <- Curry(runif, min = 0, max = 1) #distribution of Ti
#learn true quantities from dataset powerconsumption ===========================
library(simulater)
library(fda)
df <- powerconsumption
df_list <- list()
for (idx in 1:nrow(df)) {
df_list[[idx]] <- list(
't' = seq(0, 1, length.out = 1440),
'x' = unname(unlist(as.vector(df[idx,])))
)
}
n_basis <- 9  #other tested setups 7
t0_list <- seq(.1, .9, l = 40)
grid <- seq(.1, .9, l = 101)
#df_smooth <- presmoothing(df_list, t0_list = t0_list, estimate_sigma(df_list))
df_smooth <- funestim::presmoothing(df_list, t0_list)
H0 <- funestim::estimate_H0(df_smooth)
L0 <- funestim::estimate_L0(df_smooth, H0, M)
true_tibble = tibble::tibble(t = t0_list, H = H0, L = L0, sigma = sigma,
mu0 = mu0)
# H0 <- estimate_H0_order_list(df_list, t0_list, k0_list = 3,
#                              estimate_sigma(df_list))
#
# L0 <- estimate_L0_order_list(df_list, t0_list, k0_list = 3,
#                              H0_list = H0, estimate_sigma(df_list))
# true_tibble = estimate_holder_quantities(df_list, t0_list)$params
#
# H0 = true_tibble$H
# L0 = true_tibble$L
basis <- create.fourier.basis(rangeval = c(min(t0_list), max(t0_list)),
nbasis = n_basis)
H0_smooth <- smooth.basis(argvals = t0_list, y = H0, fdParobj = basis)
L0_smooth <- smooth.basis(argvals = t0_list, y = L0, fdParobj = basis)
H0_eval <- eval.fd(evalarg = grid, fdobj = as.fd(H0_smooth))[, 1]
L0_eval <- eval.fd(evalarg = grid, fdobj = as.fd(L0_smooth))[, 1]
hurst_fun <- approxfun(
x = grid, y = H0_eval,
yleft = H0_eval[1], yright = H0_eval[length(H0_eval)]
)
constant_fun <- approxfun(
x = grid, y = L0_eval,
yleft = L0_eval[1], yright = L0_eval[length(L0_eval)]
)
grid_large <- seq(0, 1, l = 10001)
A_hat_prime <- constant_fun(grid_large)**(1 / hurst_fun(grid_large))
A_hat <- pracma::cumtrapz(x = grid_large, y = A_hat_prime)
A_fun <- approxfun(
x = grid_large, y = A_hat,
yleft = A_hat[1], yright = A_hat[length(A_hat)]
)
s <- exp(-3)  # Regularity for the estimation of the mean
##other tested setups 3
hurst_fun <- approxfun(
x = grid, y = H0_eval,
yleft = H0_eval[1], yright = H0_eval[length(H0_eval)]
)
#true mean
mu_model <- learn_mean(df, k = 50)
mu <- predict_mean(grid_mu, mu_model, lambda = s, k = 50) - 240
mu_t <- tibble::tibble(t = grid_mu, mu)
#tested discretisation error on grid of 1001 points - very small
grid_cov <- seq(0, 1, length.out = 101)
pp_disto <- A_fun(grid_cov)
cov_true <- tau**2 + covariance_mfbm(grid_cov, hurst_fun, pp_disto)
#obtain the "numerically true" eigenvalues from the true covariance
evalues_true <- eigen(cov_true, symmetric = TRUE,
only.values = TRUE)$values[1:nvalues] / length(grid_cov)
efunctions_true <- eigen(cov_true, symmetric = TRUE)$vectors[, 1:nvalues] *
sqrt(length(grid_cov))
?covariance_ll
holder_estim <- estimate_holder_quantities(pfbm_mu, grid_param)
pfbm_mu <- lapply(pfbm_curves, function(curve) {
idx <- purrr::map_dbl(curve$t, ~which.min(abs(.x - mu_t$t)))
list(
t = curve$t,
x = curve$x + mu_t$mu[idx])
})
devtools::load_all()
points_list <- generates_points(N = N, m = M)
pfbm <- generate_curves(points_list, hurst_fun,
distortion_model = A_fun,
add_regular_grid_of_size = grid_size_true,
sigma = sigma,
tau = tau,
L = L)
pfbm_curves <- lapply(pfbm$curves,
function(x) list(t = x$observed$t, x = x$observed$x,
grid_true = x$ideal$t,
x_true = x$ideal$x))
pfbm_mu <- lapply(pfbm_curves, function(curve) {
idx <- purrr::map_dbl(curve$t, ~which.min(abs(.x - mu_t$t)))
list(
t = curve$t,
x = curve$x + mu_t$mu[idx])
})
holder_estim <- estimate_holder_quantities(pfbm_mu, grid_param)
holder_estim
cov_gkp <- covariance_ll(pfbm_mu, grid_bandwidth, grid_smooth, k0,
holder_estim)
eigen(cov_gkp)
eigen(cov_gkp$cov)
evalues_gkp <- eigen(cov_gkp$cov)
evlaues_gkp
evalues_gkp <- eigen(cov_gkp$cov)$values
evalues_gkp
cov_gkp$cov
persp3D(grid_smooth, grid_smooth, cov_gkp$cov)
eigen(cov_gkp$cov, symmetric = TRUE)
eigen(cov_gkp$cov, symmetric = TRUE)$values
evalues_gkp <- eigen(cov_gkp$cov, symmetric = TRUE)$values
evalues_gkp
evalues_gkp <- normalise_eigen(cov_gkp$cov, nvalues = nvalues)
evalues_gkp
evalues_gkp$values
elements_gkp <- normalise_eigen(cov_gkp$cov, nvalues = nvalues)
evalues_gkp <- elements_gkp$values
efunctions_gkp <- elements_gkp$vectors
efunctions_gkp
evalues_pw <- evalues_adaptive(pfbm_mu, grid_bandwidth, grid_smooth,
k0, nvalues, holder_estim)
evalues_pw
evalues_true
evalues_adaptive
evalues_gkp
nfunctions
nfunctions = 10
efunctions_pw <- efunctions_adaptive(pfbm_mu, grid_bandwidth, grid_smooth,
k0, nfunctions, params)
efunctions_pw <- efunctions_adaptive(pfbm_mu, grid_bandwidth, grid_smooth,
k0, nfunctions, holder_estim)
cov_zw <- covariance_lll(pfbm_mu, grid_smooth)
evalues_zw <- normalise_eigen(cov_zw, nvalues)
elements_zw <- normalise_eigen(cov_zw, nvalues)
evalues_zw <- elements_zw$values
efunctions_zw <- elements_zw$vectors
evalues_gkp
source("/Users/swang/Dropbox/FPCA/simuls/nice_dgp/dgp.R")
#source("/Users/swang/Dropbox/Mac/Downloads/juin15/codes_val/dgp_old.R")
# source("/Users/swang/Dropbox/funeigen/R/H_and_L_functions.R")
# source("/Users/swang/Dropbox/funeigen/R/mean_optimised.R")
# source("/Users/swang/Dropbox/funeigen/R/cov_optimised.R")
# source("/Users/swang/Dropbox/funeigen/R/utils.R")
# source("/Users/swang/Dropbox/funeigen/R/eigenvalues.R")
library(foreach)
library(fs)
library(doParallel)
library(functional)
library(purrr)
#parameters
N <- 200
M <- 25
#H <- c(0.5, 0.8)
sigma <- 0.5
L <- 1
mu0 <- 1
grid_size_true <- 101
grid_true <- seq(0, 1, length.out = grid_size_true)
grid_mu <- seq(0, 1, length.out = 1001)
grid_smooth <- seq(0, 1, length.out = 101)
grid_bandwidth <- lseq(0.001, 0.1, length.out = 51)
grid_param = seq(0.1, 0.9, length.out = 20)
k0 <- 1
n_simu <- 45
# k_length <- 100
# alpha <- 2
#change_point <- 0.5
#slope <- 50
tau <- 2.5 #variance of random starting points
# scale <- 1
# shift <- 0
nvalues <- 10
nfunctions <- 10
points_dist <- Curry(runif, min = 0, max = 1) #distribution of Ti
#learn true quantities from dataset powerconsumption ===========================
library(simulater)
library(fda)
df <- powerconsumption
df_list <- list()
for (idx in 1:nrow(df)) {
df_list[[idx]] <- list(
't' = seq(0, 1, length.out = 1440),
'x' = unname(unlist(as.vector(df[idx,])))
)
}
n_basis <- 9  #other tested setups 7
t0_list <- seq(.1, .9, l = 40)
grid <- seq(.1, .9, l = 101)
#df_smooth <- presmoothing(df_list, t0_list = t0_list, estimate_sigma(df_list))
df_smooth <- funestim::presmoothing(df_list, t0_list)
H0 <- funestim::estimate_H0(df_smooth)
L0 <- funestim::estimate_L0(df_smooth, H0, M)
true_tibble = tibble::tibble(t = t0_list, H = H0, L = L0, sigma = sigma,
mu0 = mu0)
# H0 <- estimate_H0_order_list(df_list, t0_list, k0_list = 3,
#                              estimate_sigma(df_list))
#
# L0 <- estimate_L0_order_list(df_list, t0_list, k0_list = 3,
#                              H0_list = H0, estimate_sigma(df_list))
# true_tibble = estimate_holder_quantities(df_list, t0_list)$params
#
# H0 = true_tibble$H
# L0 = true_tibble$L
basis <- create.fourier.basis(rangeval = c(min(t0_list), max(t0_list)),
nbasis = n_basis)
H0_smooth <- smooth.basis(argvals = t0_list, y = H0, fdParobj = basis)
L0_smooth <- smooth.basis(argvals = t0_list, y = L0, fdParobj = basis)
H0_eval <- eval.fd(evalarg = grid, fdobj = as.fd(H0_smooth))[, 1]
L0_eval <- eval.fd(evalarg = grid, fdobj = as.fd(L0_smooth))[, 1]
hurst_fun <- approxfun(
x = grid, y = H0_eval,
yleft = H0_eval[1], yright = H0_eval[length(H0_eval)]
)
constant_fun <- approxfun(
x = grid, y = L0_eval,
yleft = L0_eval[1], yright = L0_eval[length(L0_eval)]
)
grid_large <- seq(0, 1, l = 10001)
A_hat_prime <- constant_fun(grid_large)**(1 / hurst_fun(grid_large))
A_hat <- pracma::cumtrapz(x = grid_large, y = A_hat_prime)
A_fun <- approxfun(
x = grid_large, y = A_hat,
yleft = A_hat[1], yright = A_hat[length(A_hat)]
)
s <- exp(-3)  # Regularity for the estimation of the mean
##other tested setups 3
hurst_fun <- approxfun(
x = grid, y = H0_eval,
yleft = H0_eval[1], yright = H0_eval[length(H0_eval)]
)
#true mean
mu_model <- learn_mean(df, k = 50)
mu <- predict_mean(grid_mu, mu_model, lambda = s, k = 50) - 240
mu_t <- tibble::tibble(t = grid_mu, mu)
#tested discretisation error on grid of 1001 points - very small
grid_cov <- seq(0, 1, length.out = 101)
pp_disto <- A_fun(grid_cov)
cov_true <- tau**2 + covariance_mfbm(grid_cov, hurst_fun, pp_disto)
#obtain the "numerically true" eigenvalues from the true covariance
evalues_true <- eigen(cov_true, symmetric = TRUE,
only.values = TRUE)$values[1:nvalues] / length(grid_cov)
efunctions_true <- eigen(cov_true, symmetric = TRUE)$vectors[, 1:nvalues] *
sqrt(length(grid_cov))
nvalues
nfunctions
library(foreach)
library(fs)
library(doParallel)
intermediate_directory <- './intermediate'
if (!dir.exists(intermediate_directory)){
dir.create(intermediate_directory)
}
cl <- parallel::makeCluster(5)
doParallel::registerDoParallel(cl)
tictoc::tic()
foreach::foreach(i = 1:n_simu,
.packages = c("dplyr", "MASS", "rpart", "matrixStats",
"purrr", "functional")) %dopar%
{
each_filename <- paste0('RESULT_N', N, '_M', M, '_', as.character(i),
'.rda')
each_filepath <- file.path(intermediate_directory, each_filename)
if (file.exists(each_filepath)) {
next
}
devtools::load_all()
points_list <- generates_points(N = N, m = M)
pfbm <- generate_curves(points_list, hurst_fun,
distortion_model = A_fun,
add_regular_grid_of_size = grid_size_true,
sigma = sigma,
tau = tau,
L = L)
pfbm_curves <- lapply(pfbm$curves,
function(x) list(t = x$observed$t, x = x$observed$x,
grid_true = x$ideal$t,
x_true = x$ideal$x))
pfbm_mu <- lapply(pfbm_curves, function(curve) {
idx <- purrr::map_dbl(curve$t, ~which.min(abs(.x - mu_t$t)))
list(
t = curve$t,
x = curve$x + mu_t$mu[idx])
})
holder_estim <- estimate_holder_quantities(pfbm_mu, grid_param)
cov_gkp <- covariance_ll(pfbm_mu, grid_bandwidth, grid_smooth, k0,
holder_estim)
elements_gkp <- normalise_eigen(cov_gkp$cov, nvalues = nvalues)
evalues_gkp <- elements_gkp$values
efunctions_gkp <- elements_gkp$vectors
evalues_pw <- evalues_adaptive(pfbm_mu, grid_bandwidth, grid_smooth,
k0, nvalues, holder_estim)
efunctions_pw <- efunctions_adaptive(pfbm_mu, grid_bandwidth, grid_smooth,
k0, nfunctions, holder_estim)
cov_zw <- covariance_lll(pfbm_mu, grid_smooth)
elements_zw <- normalise_eigen(cov_zw, nvalues)
evalues_zw <- elements_zw$values
efunctions_zw <- elements_zw$vectors
result <- list(
'evalues_gkp' = evalues_gkp,
'efunctions_gkp' = efunctions_gkp,
'evalues_zw' = evalues_zw,
'efunctions_zw' = efunctions_zw,
'evalues_pw' = evalues_pw$eigenvalues,
'efunctions_pw' = efunctions_pw$eigenfunctions,
'pw_bandwidth_eval' = evalues_pw$bandwidth,
'pw_bandwidth_efun' = efunctions_pw$bandwidth
)
save(result, file = each_filepath)
}
n_simu
n_simu = 18
R -e '.libPaths()'
.libPaths()
.libPaths()
?install_github
remotes::install_github("https://github.com/sunnywang93/funeigen", auth_token = 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)
remotes::install_github("https://github.com/sunnywang93/funeigen", auth_token = "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")
devtools::install_github("sunnywang93/funeigen"
,ref="main"
,auth_token = "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"
)
?funeigne
?funeigen
?covariance_ll
devtools::install_local(path = "/Users/swang/Dropbox/funeigen")
devtools::install_local(path = "/Users/swang/Dropbox/funeigen")
devtools::install_local(path = "/Users/swang/Dropbox/funeigen", force = TRUE)
document()
document()
devtools::install_local(path = "/Users/swang/Dropbox/funeigen", force = TRUE)
document()
devtools::install_local(path = "/Users/swang/Dropbox/funeigen", force = TRUE)
estimate_variance_curves
document()
devtools::install_local(path = "/Users/swang/Dropbox/funeigen", force = TRUE)
document()
devtools::install_local(path = "/Users/swang/Dropbox/funeigen", force = TRUE)
source("/Users/swang/Dropbox/FPCA/simuls/nice_dgp/dgp.R")
#source("/Users/swang/Dropbox/Mac/Downloads/juin15/codes_val/dgp_old.R")
# source("/Users/swang/Dropbox/funeigen/R/H_and_L_functions.R")
# source("/Users/swang/Dropbox/funeigen/R/mean_optimised.R")
# source("/Users/swang/Dropbox/funeigen/R/cov_optimised.R")
# source("/Users/swang/Dropbox/funeigen/R/utils.R")
# source("/Users/swang/Dropbox/funeigen/R/eigenvalues.R")
library(foreach)
library(fs)
library(doParallel)
library(functional)
library(purrr)
#parameters
N <- 200
M <- 25
#H <- c(0.5, 0.8)
sigma <- 0.5
L <- 1
mu0 <- 1
grid_size_true <- 101
grid_true <- seq(0, 1, length.out = grid_size_true)
grid_mu <- seq(0, 1, length.out = 1001)
grid_smooth <- seq(0, 1, length.out = 101)
grid_bandwidth <- lseq(0.001, 0.1, length.out = 51)
grid_param = seq(0.1, 0.9, length.out = 20)
k0 <- 1
n_simu <- 45
# k_length <- 100
# alpha <- 2
#change_point <- 0.5
#slope <- 50
tau <- 2.5 #variance of random starting points
# scale <- 1
# shift <- 0
nvalues <- 10
nfunctions <- 10
points_dist <- Curry(runif, min = 0, max = 1) #distribution of Ti
#learn true quantities from dataset powerconsumption ===========================
library(simulater)
library(fda)
df <- powerconsumption
df_list <- list()
for (idx in 1:nrow(df)) {
df_list[[idx]] <- list(
't' = seq(0, 1, length.out = 1440),
'x' = unname(unlist(as.vector(df[idx,])))
)
}
n_basis <- 9  #other tested setups 7
t0_list <- seq(.1, .9, l = 40)
grid <- seq(.1, .9, l = 101)
#df_smooth <- presmoothing(df_list, t0_list = t0_list, estimate_sigma(df_list))
df_smooth <- funestim::presmoothing(df_list, t0_list)
H0 <- funestim::estimate_H0(df_smooth)
L0 <- funestim::estimate_L0(df_smooth, H0, M)
true_tibble = tibble::tibble(t = t0_list, H = H0, L = L0, sigma = sigma,
mu0 = mu0)
# H0 <- estimate_H0_order_list(df_list, t0_list, k0_list = 3,
#                              estimate_sigma(df_list))
#
# L0 <- estimate_L0_order_list(df_list, t0_list, k0_list = 3,
#                              H0_list = H0, estimate_sigma(df_list))
# true_tibble = estimate_holder_quantities(df_list, t0_list)$params
#
# H0 = true_tibble$H
# L0 = true_tibble$L
basis <- create.fourier.basis(rangeval = c(min(t0_list), max(t0_list)),
nbasis = n_basis)
H0_smooth <- smooth.basis(argvals = t0_list, y = H0, fdParobj = basis)
L0_smooth <- smooth.basis(argvals = t0_list, y = L0, fdParobj = basis)
H0_eval <- eval.fd(evalarg = grid, fdobj = as.fd(H0_smooth))[, 1]
L0_eval <- eval.fd(evalarg = grid, fdobj = as.fd(L0_smooth))[, 1]
hurst_fun <- approxfun(
x = grid, y = H0_eval,
yleft = H0_eval[1], yright = H0_eval[length(H0_eval)]
)
constant_fun <- approxfun(
x = grid, y = L0_eval,
yleft = L0_eval[1], yright = L0_eval[length(L0_eval)]
)
grid_large <- seq(0, 1, l = 10001)
A_hat_prime <- constant_fun(grid_large)**(1 / hurst_fun(grid_large))
A_hat <- pracma::cumtrapz(x = grid_large, y = A_hat_prime)
A_fun <- approxfun(
x = grid_large, y = A_hat,
yleft = A_hat[1], yright = A_hat[length(A_hat)]
)
s <- exp(-3)  # Regularity for the estimation of the mean
##other tested setups 3
hurst_fun <- approxfun(
x = grid, y = H0_eval,
yleft = H0_eval[1], yright = H0_eval[length(H0_eval)]
)
#true mean
mu_model <- learn_mean(df, k = 50)
mu <- predict_mean(grid_mu, mu_model, lambda = s, k = 50) - 240
mu_t <- tibble::tibble(t = grid_mu, mu)
#tested discretisation error on grid of 1001 points - very small
grid_cov <- seq(0, 1, length.out = 101)
pp_disto <- A_fun(grid_cov)
cov_true <- tau**2 + covariance_mfbm(grid_cov, hurst_fun, pp_disto)
#obtain the "numerically true" eigenvalues from the true covariance
evalues_true <- eigen(cov_true, symmetric = TRUE,
only.values = TRUE)$values[1:nvalues] / length(grid_cov)
efunctions_true <- eigen(cov_true, symmetric = TRUE)$vectors[, 1:nvalues] *
sqrt(length(grid_cov))
